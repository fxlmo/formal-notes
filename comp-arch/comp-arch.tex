%%=====================================================================================
%%
%%       Filename:  comp-arch.tex
%%
%%    Description:  A comprehensive list of notes from teaching block 1, year 1.
%%
%%        Version:  1.0
%%        Created:  16/12/18
%%
%%         Author:  Josh Felmeden (), nk18044@bristol.ac.uk
%%      Copyright:  Copyright (c) 2018, Josh Felmeden
%%
%%
%%=====================================================================================

% Preamble
\documentclass[11pt,a4paper,titlepage,dvipsnames,cmyk]{scrartcl}
\usepackage[english]{babel}
\typearea{12}

% Set indentation and line skip for paragraph
\setlength{\parindent}{0em}
\setlength{\parskip}{1em}
\usepackage[margin=2cm]{geometry}
\addtolength{\textheight}{-1in}
\setlength{\headsep}{.5in}

% Headers setup
\usepackage{fancyhdr}
\pagestyle{fancy}
\lhead{Introduction to Computer architecture: The Comprehensive Notes}
\rhead{Josh Felmeden}
\usepackage{hyperref} 
\usepackage{mathtools} 


% Listings
\usepackage[]{listings,xcolor} 
\lstset
{
    breaklines=true,
    tabsize=3,
    showstringspaces=false
}


\lstdefinestyle{Common}
{
    extendedchars=\true,
    language={[Visual]Basic},
    frame=single,
    %===========================================================
    framesep=3pt,%expand outward.
    framerule=1.4pt,%expand outward.
    xleftmargin=3.4pt,%make the frame fits in the text area. 
    xrightmargin=3.4pt,%make the frame fits in the text area.
    %=========================================================== 
    rulecolor=\color{Red}
}

\lstdefinestyle{A}
{
    style=Common,
    backgroundcolor=\color{Yellow!10},
    basicstyle=\small\color{Black}\ttfamily,
    keywordstyle=\color{Orange},
    identifierstyle=\color{Cyan},
    stringstyle=\color{Red},
    commentstyle=\color{Green}
}

\lstdefinestyle{B}
{
    style=Common,
    backgroundcolor=\color{Black},
    basicstyle=\scriptsize\color{White}\ttfamily,
    keywordstyle=\color{Orange},
    identifierstyle=\color{Cyan},
    stringstyle=\color{Red},
    commentstyle=\color{Green}
}

\usepackage[]{amsmath} 
\usepackage[]{booktabs} 
\usepackage[symbol]{footmisc} 
\renewcommand{\thefootnote}{\fnsymbol{footnote}}

% Title
\title{Introduction to Computer Architecture: The (almost) Comprehensive
Notes}
\date{2018\\ December}
\author{Josh Felmeden}

% Start document
\begin{document}
\pagenumbering{roman}
\maketitle

\tableofcontents
\newpage
\pagenumbering{arabic}

\section{Integer representation and arithmetic}%
\label{sec:reallyboring}
First things first, we need to look at the ways that numbers are added together
(I chose to skip over how numbers are stored in the computer because I think
this is boring and if you need help with this then you really are beyond all
hope.). Ultimately, we've started with addition to start with, so we're going to
look at simple circuits for addition in a computer. By the way, if we have a
letter with a hat on (namely: $\hat x$), this means that it's a bit sequence
representing some integer $x$. This leaves us with this:
\begin{align*}
    \hat x \mapsto x \\
    \hat y \mapsto y \\
    \hat r \mapsto r
\end{align*}

Alongside this, we have the following relationship:
\begin{equation}
    r = x + y
\end{equation}

The question is, how do we take this and represent it using boolean algebra?
Well, we're going to try:
\begin{equation}
    \hat r = F(\hat x, \hat y)
\end{equation}

Where $F$ is some boolean expression. What this means is that the $+$ operator
has a similar result than $F$. What the bloody hell is this function? Let's have
a look.

It's actually not that bad. If we look at how humans do addition, we have:
\begin{align*}
    &\hat x = 1 \quad 0 \quad 7 \\
    + \quad &\hat y = 0 \quad 1 \quad 4 \\
        &\rule{60pt}{1pt} \\
           &c = 0 \quad 1 \quad 0 \\
           &\hat r = 1 \quad 2 \quad 1
\end{align*}
Where c is the carry (we also have 0 as a carry out here). The same thing can be
represented in binary:
\begin{align*}
    107_{10} = \quad &\hat x = 0 \quad 1 \quad 1 \quad 0 \quad 1 \quad 0 \quad 1 \quad
    1 \\
    + 14_{10} = \quad &\hat y = 0 \quad 0 \quad 0 \quad 0 \quad 1 \quad 1 \quad 1
    \quad 0 \\
      &\rule{150pt}{1pt} \\
      &c = 0 \quad 0 \quad 0 \quad 1 \quad 1 \quad 1 \quad 0 \quad 0 \\
      &\hat r = 0 \quad 1 \quad 1 \quad 1 \quad 1 \quad 0 \quad 0 \quad 1
\end{align*}

Now, we're going to create a really simple algorithm (it's going to be called
ADD), and is going to look like $\text{ADD}(\hat x, \hat y, n, b, ci)$, where b
is the base, ci is the carry in and n is the length of x and y. We would then
have the algorithm as follows:
\begin{lstlisting}[style=B]
for i = 0 to (n-1)
    r(i) += (x(i) + y(i) + c(i)) mod b
    if(x(i) + y(i) + c(i) < b)
        c(i+1) = 0
    else
        c(i+1) = 1
    end if
next
co = c(n)
return r, co
\end{lstlisting}

Let's step through this algorithm:
\begin{gather*}
    \hat x = \langle 7, 0, 1 \rangle \mapsto 107_{10} \\
    \hat y = \langle 4, 1, 0 \rangle \mapsto 14_{10} \\
    n = 3, \ b = 10, \ ci = 0, \\
    \text{ADD}(\hat x, \hat, y, 3, 10, 0)
\end{gather*}

\begin{center}
    \begin{tabular}{|c|c|c|c|}
        \hline
        i & $\hat x_i, \ \hat y_i, \ c_i$ & $\hat x_i + \hat y_i + c_i$ &
        $c_{i+1}, \hat r_i$\\
        \hline
        0 & 7, 4, 0 & 11 & 1, 1 \\
        \hline
        1 & 0, 1, 1 & 2 & 0, 2 \\
        \hline
        2 & 1, 0, 0 & 1 & 0, 1 \\
        \hline
    \end{tabular}
\end{center}

Where the at the end, $\hat r = \langle 1 2 1 \rangle$, as stated by the last
column.

In the algorithm above, the bit inside the for loop can be represented by $F_i$,
where it has the inputs $\hat x_i, \hat y_i, \hat c_i$ and has outputs 
$\hat r_i,c_{xi+1}$. We don't have to know what the function is, but we can write
down their behaviour. Because we know what should happen\footnote[1]{These tables
took so damn long please appreciate them}.

\begin{center}
    \begin{tabular}{|c c c|c|c|}
        \hline
        $c_i$ & $\hat x_i$ & $\hat y_i$ & $c_{i+1}$ & $\hat r_i$ \\
        \hline
        0 & 0 & 0 & 0 & 0 \\
        \hline
        0 & 0 & 1 & 0 & 1 \\
        \hline
        0 & 1 & 0 & 0 & 1 \\
        \hline
        0 & 1 & 1 & 1 & 0 \\
        \hline
        1 & 0 & 0 & 0 & 1 \\
        \hline
        1 & 0 & 1 & 1 & 0 \\
        \hline
        1 & 1 & 0 & 1 & 0 \\
        \hline
        1 & 1 & 1 & 1 & 1 \\
        \hline
    \end{tabular}
\end{center}

We know that $c_{i+1} = (\hat x_i \wedge \hat y_i) \vee (\hat x_i \wedge c_i)
\vee (\hat y_i \wedge c_i)$ and we ALSO know that $\hat r_i = \hat x_i \oplus
\hat y_i \oplus c_i$. Thus ends the middle bit of the algorithm, so now let's
look at the whole algorithm.

Basically, we just string together a load of the components that we had before.
That is to say, if we have $n$ bits, we need $n$ of those adders. Each adder
takes 3 inputs: the $n$th digit of $x$ and $y$, and a carry, which comes from
the $n-1$th adder. At the very end of the block, we get a carry out.
Additionally, at each adder, we get the $n$th digit of the result ($\hat r$).
This could kinda be obvious, but if you think that, shut up. It's interesting to
know that there is not computation other than that in the adders. This is called
a \textbf{ripple carry adder} and it relates to the loop within the algorithm.
Each one of the `components' that I was talking about is called a \textbf{full
adder}, but we might replace this with a half adder later (we definitely will).
We can write this whole thing in forms of boolean expressions now, so there you
go.

Before we finish with these bad boys, we need to explore some examples. Let's
look at when $\hat x = 1111$ and $\hat y = 0001$. If we want to add those
together, we get:
\begin{align*}
    &\hat x = 1 \quad 1 \quad 1 \quad 1 \mapsto 15_{10} \\
    + \quad &\hat y = 0 \quad 0 \quad 0 \quad 1 \mapsto 1_{10} \\
         &\rule{75pt}{1pt}\\
         &c = 1 \quad 1 \quad 1 \quad 0 \quad (c_o = 1)\\
    &\hat r = 0 \quad 0 \quad 0 \quad 0
\end{align*}

This is an issue because $15 + 1$ is NOT 0, and this is an error. We use the
carry out to determine whether there has been an error, because if there is a
carry out of 1, then there is clearly an error.

If we look at the case when we use all 1s with 2's complement (i.e. -1), and we
add 1 to it, happily we get 0. We have the same behaviour, but the result is
right. Unfortunately, we lost the functionality of the 1 as a carry out error
marker, because there are still the possibility for errors. Take, for example,
$x = 0111_2 \mapsto 7_{10}$ and $y = 0001_2 \mapsto 1_{10}$. When we add these in
binary, we end up with $1000_2 \mapsto -8_{10}$, with a carry out of 0. This is
not the right answer. There is kind of a way around this, where if there are a
mismatch between the first bit of x and y and the first (most significant) bit
of c and r. Formally then, the sign of $\hat x, \hat y, \hat r$ should not end
up with a $+ve + +ve \rightarrow -ve$ and vice versa. This is known as a
\textit{carry error}. 

With the errors that we have detected, we should be responsible people and tell
the programmers that an error has occurred by medium of a flag, or even CORRECT
the error, (but we can't do that because we have a fixed number of bits).

\section{Transistors}%
\label{sec:Transistors}

An electrical current is a \textit{flow} of electrons. A capacitor (such as a
battery) works by having \textbf{free electrons} move from high to low
potential. A \textit{conductivity} rating says how easily electrons can move.
\begin{itemize}
    \item A conductor has \textit{high conductivity} and allows electrons to
        move easily.
    \item An insulator has low conductivity and does \textbf{not} allow
        electrons to move easily.
\end{itemize}

Silicon is a DOPE material, because there's lots of it, and it's also pretty
cheap. It's also inert (which means boring aka doesn't react in weird ways)
because it's stable enough to not react in weird ways with normal things
\textit{and} it can be doped with a donor material, which will allow us to
construct the materials with the precise sub atomic properties that we want.

The result of this is a semi conductor. `What is this?' I hear you ask. Well,
it's kind of a conductor and kind of not. If this isn't any clearer, here's a
little more info:
\begin{itemize}
    \item A \textbf{P-type} semi-conductor has extra holes, while
        \textbf{N-type} has extra electrons.
    \item if we sandwich together the P and N type layers together, the result
        is that the electrons can only move in one way. For example, from N to
        P, but not vice versa.
\end{itemize}

Back in the olden days, we used to have a vacuum tube, because when the filament
heats up, the electrons are produced into the vacuum, which are then attracted
by the plate. They're pretty reliable generally, but they fail a fail bit during
power on and off. It's also where we get the term \textit{bug} from, since a
literal bug could cause failures in this thing.

\subsection{MOSFETs}%
\label{sub:MOSFETs}
We're now going to look at MOSFETS gang. MOSFET stands for Metal Oxide
Semi-conductor Field Effect Transistor. Yeah, really. That's why there is an
abbreviation for it. A MOSFET has 3 parts, a \textbf{source}, \textbf{drain},
and a \textbf{gate}. The source and the drain are terminals, and the gate is
what controls the flow of electrons between the source and the drain. That, on a
simple level, is that, because any further description is pretty freaking
complicated and is not necessary for this course.

\subsubsection{N-MOSFET}%
\label{ssub:N-MOSFET}
An N-MOSFET (or negative MOSFET) is constructed from \textbf{n-type}
semiconductor terminals inside a p-type body. This means that applying a
potential difference to
the gate \textit{widens} the conductive channel, meaning that the source and
drain are connected, and the transistor is activated. Removing the potential
difference
\textit{narrows} the conductive channel and the source and the drain are
disconnected. Simply, \textbf{p.d. = current flows through, else block}.

\subsubsection{P-MOSFET}%
\label{ssub:P-MOSFET}
A P-MOSFET (or positive MOSFET) is constructed from \textbf{p-type}
semiconductor terminals inside an n-type body. Applying a potential difference to the gate
\textit{narrows} the conductive channel, meaning that no current can flow, and
removing the potential difference allows the current to flow. Simply, \textbf{p.d. = no
current flowing, else there is current flowing}. Also, p-types have a funny
looking bobble hat in a diagram.

These MOSFETS aren't normally used in isolation, and they are used in CMOS
cells, which stands for complimentary metal oxide semiconductor. We combine 2 of
one type and one of the other into one body, namely, the CMOS cell. It's pretty
useful because they work in complimentary ways, but there is also little
leakage, or \textbf{static} power consumption. It only consumes power during the
switching action (\textbf{dynamic} consumption).

\subsection{Manufacture}%
\label{sub:Manufacture}
It's necessary to be able to construct these bad boys in batch, because
otherwise we wouldn't be able to make big machines out of them because we need
so many of them. What we do is:
\begin{enumerate}
    \item Start with a clean, prepared \textbf{wafer}.
    \item Apply a layer of \textbf{substrate} material, such as a metal or a
        semi conductor.
    \item Apply a layer of photoresist. This material reacts differently when it
        is exposed to light.
    \item To do this, we expose a precise negative (or \textit{mask}) of design
        that hardens the exposed photoresist.
    \item Wash away the unhardened photoresist.
    \item Etch away the uncovered substrate.
    \item Strip away the hardened photoresist.
\end{enumerate}

Remember that this algorithm repeats over and over in order to make the result 3
dimensions, rather than 2. Regularity is a huge advantage because we can
manufacture a great number of similar components in a layer using a single
process. The feature size (it's 90nm big) relates to the resolution of the
process.

These components are USELESS in this form, so they're packaged before use, which
protects against damage, including heat sinks and an interface between the
component and the outside world using pins bonded to internal inputs and
outputs.

So, while MOSFETs are pretty great, there are down sides. Designing
complex functionality using transistors alone is really hard because
transistors are simply \textit{too} low level. We can address this problem
by repackaging groups of transistors into logic gates, since logic gates
are ordered logic gates such that we get certain functionality. Pretty
cool right? It's like nerdy Lego.

\section{Logic Gates}%
\label{sec:logic-gates}
If we form a \textbf{pull-up network} of P-MOSFET transistors, connected
to $V_{dd}$ (which is the high voltage rail), and a \textbf{pull-down
network} of N-MOSFETs, connected to $V_{ss}$ (the low voltage rail), and
assume that the power rails are everywhere:
\begin{align*}
    V_{ss} &= 0V \approx 0 \\
    V_{dd} &= 5V \approx 1
\end{align*}

We can then describe the operation of each logic gate using a truth table.

\subsection{NAND gate}%
\label{sub:nand}
% Insert picture of a nand gate I think
If both x and y are 0 (connected to $V_{ss}$), then:
\begin{enumerate}
    \item Both top P-MOSFETs will be connected.
    \item Both bottom N-MOSFETs will be disconnected.
    \item r (the output) will be connected to $V_{dd}$
\end{enumerate}

If x is 1 and y is 0, then:
\begin{enumerate}
    \item The right most P-MOSFET will be connected.
    \item The upper-most N-MOSFET will be disconnected.
    \item r will be connected to $V_{dd}$
\end{enumerate}

If x is 0 and y is 1, then:
\begin{enumerate}
    \item The left most P-MOSFET will be connected.
    \item The lower-most N-MOSFET will be disconnected.
    \item r will be connected to $V_{dd}$
\end{enumerate}

Finally, if both x and y are 1, then:
\begin{enumerate}
    \item Both top P-MOSFETs will be disconnected.
    \item Both bottom N-MOSFETs will be connected.
    \item r will be connected to $V_{ss}$
\end{enumerate}

\subsection{NOR gate}%
\label{sub:nor}
% Insert picture of a nor gate
If both x and y are 0, then:
\begin{enumerate}
    \item Both top P-MOSFETs will be connected.
    \item Both bottom N-MOSFETs will be disconnected.
    \item r will be connected to $V_{dd}$
\end{enumerate}

If x is 1 and y is 0, then:
\begin{enumerate}
    \item The upper-most P-MOSFET will be disconnected
    \item the left-most N-MOSFET will be connected.
    \item r will be connected to $V_{ss}$
\end{enumerate}

If x is 0 and y is 1, then:
\begin{enumerate}
    \item The lower-most P-MOSFET will be disconnected.
    \item The right-most N-MOSFET will be connected.
    \item r will be connected to $V_{ss}$.
\end{enumerate}

If both x and y are 1, then:
\begin{enumerate}
    \item Both top P-MOSFETs will be disconnected.
    \item Both bottom N-MOSFETs will be connected.
    \item r will be connected to $V_{ss}$.
\end{enumerate}

\subsection{Physical limitations}%
\label{sub:Physical limitations}
There are, of course, some physical limitations that we haven't discussed
yet. There are two classes of delay (often described as
\textbf{propagation delay}), that will dictate the time between change to
some input and corresponding change in an output. These are:
\begin{itemize}
    \item \textbf{Wire delay:} This relates to the time taken for the
        current to move through the conductive wire from one point to
        another.
    \item \textbf{Gate delay:} This relates to the time taken for the
        transistors in each gate to switch between the connected and
        disconnected states
\end{itemize}
Normally, the gate delay is more than wire delay, and both relate to the
implementations. Gate delay is the fault of the properties of the
transistors used, and wire delay to the properties of the wire.

\textbf{Critical path} is the longest sequential delay possible in some
combinatorial logic. Basically, the worst case scenario in a given logic
circuit.

Ideally, we'd get a perfect digital on/off graph of the response, while in
reality, we get a more curved graph. It would also make sense to have 1
and 0 as a threshold, rather than a definitive voltage. This fuzzy
representation allows for some inaccuracies that are unavoidable with this
physical implementation.

Including gate delay gives us a \textit{dynamic} view computation. We're
gonna kind of ignore wire delay for the moment and arbitrarily choose some
delays for the gates:
\begin{enumerate}
    \item NOT: 10ns
    \item AND: 20ns
    \item OR: 20ns
\end{enumerate}

If we then did some computation, we could see the output changing after we
switch x from 0 to 1 and keep y as 1. If we're using an XOR gate, then it
would take 50ns to completely change to 0 (since x and y are both 1, hence
x XOR y = 0).

It is cool to use \textit{3-stage logic}, using an extra value:
\begin{itemize}
    \item 0 is false
    \item 1 is true
    \item Z is \textbf{high impedance}
\end{itemize}

High impedance is the null value, so we can allow a wire to be
disconnected.

If we have two inputs connected to an output, you can have some weird
results where the two inputs are in conflict with the other. The way to
fix this is to have some enable switch on the two inputs to the output, so
we don't get any inconsistencies.

Here are some more definitions:
\begin{itemize}
    \item \textbf{fan-in} is used to describe the number of inputs to a
        given gate.
    \item \textbf{fan-out} is used to describe the number of inputs (or
        \textit{other} gates) the output of a given gate is connected to.
\end{itemize}

\section{Combinatorial Logic}%
\label{sec:combinatorial}

The logic gates that we've discussed already are higher level than the
transistors that we started off with. Armed with these new logic gates, we
can move onto \textit{actual} high level components. We want components
that are closer to what we can actually do things with.

Thus far, we've just looked at various ways of writing the same things;
having moved from the completely abstract pencil and paper, to the
implementation of logic gates and NAND-boards.

\subsection{Design patterns}%
\label{sub:patterns}
There are a number of design patterns that we can take:

\subsubsection{Decomposition}%
\label{ssub:Decomposition}

Divide and conquer, take some complicated design, and break it down into
simpler and more manageable parts.

\subsubsection{Sharing}%
\label{ssub:Sharing}

We can replace two AND gates that exist in some design with a single AND
gate, using the same gate from the usage points.  It makes sense because
the output will always be the same.

\subsubsection{Isolated Replication}%
\label{ssub:isolated}

Say we have a 2 input AND gate, and we want a 2-input \textit{m}-bit AND
gate, which is simply a replication of 2-input, 1-bit AND gates, then we
are going to have \textit{m}-bits of the AND gate. We're also saying that
they are operating in parallel, and that each bit does not affect the
neighbouring bits.

\subsubsection{Cascaded Replication}%
\label{ssub:cascaded}

Using the same example as before, instead of using \textit{m} AND gates,
we want something like this:
\begin{alignat*}{4}
    r &= & &x_0 \wedge x_1   &&\ \wedge \ & &x_2 \wedge x_3 \\
      &= (& &x_0 \wedge x_1) &&\wedge \ (& &x_2 \wedge x_3)
\end{alignat*}

You can also write that like this:
\begin{equation*}
    r = \bigwedge^{n-1}_{i=0} x_i
\end{equation*}

It's different from isolated replication, because the and gates are
entirely isolated from one another.

\subsection{Mechanical Derivation}%
\label{sub:mechanical}
In every case, the process is the same. We want to be able to process some
truth table and get something that will work (with a boolean expression).
So, let's let $T_i$ denote the $j$th input for $0 \le j < n$, and let $O$
denote the single output.

\begin{enumerate}
    \item Find a set $T$ such that $i \in T$ iff. $O = 1$ in the $i$th row
        of the truth table.
    \item For each $i \in T$, form a term $t_i$ by ANDing together all the
        variables while following two rules:
        \begin{enumerate}
            \item if $T_j = 1$ in the $i$th row, then we use $T_j$ as is,
                but
            \item If $T_j = 0$ in the $i$th row, then we use $-T_j$
        \end{enumerate}
    \item An expression implementing this function is then formed by ORing
        all of the terms together:
        \begin{equation*}
            e = \bigvee_{i \in T} t_i
        \end{equation*}
\end{enumerate}

Let's consider the example of deriving an expression for XOR:
\begin{equation*}
    r = f(x,y) = x \oplus y
\end{equation*}

This is a function described by the following table:
\begin{center}
    \begin{tabular}{|c c|c|}
        \hline
        $x$ & $y$ & $r$ \\
        \hline
        0 & 1 & 1 \\
        \hline
        1 & 0 & 1 \\
        \hline
        1 & 1 & 0 \\
        \hline
    \end{tabular}
\end{center}

\subsection{Karnaugh Map}%
\label{sub:karnaugh}

The simple algorithm for creating a Karnaugh map is as follows:
\begin{enumerate}
    \item Draw a rectangular ($p \times 1$) element grid:
    \begin{enumerate}
        \item $p = q = 0 (mod 2)$, and
        \item $p \cdot q = 2^n$
    \end{enumerate}
    \item Fill the grid elements with the output that corresponds to
        inputs for that row and column.
    \item Cover rectangular groups of
        adjacent 1 elements which are of total size $2^n$ for some group
        $m$, groups can `wrap around' if you like, so they can go over
        edges of the grid and overlap.
    \item Translate each group into a single term in some SoP form Boolean
        expression, where
    \begin{enumerate}
        \item bigger groups
        \item less groups
    \end{enumerate}
    mean a simpler expression.
\end{enumerate}

The basic idea is that we don't count up as per normal in binary, but we
actually count up by changing one binary digit at a time. You then group
them, and the groups have to be adjacent (but they can't form a kind of L
shape). After this, it's easy to group them into some format.

\subsection{Building blocks}%
\label{sub:blocks}

There are two more blocks that we're going to look at now:
\begin{enumerate}
    \item \textbf{Multiplexer}
        \begin{itemize}
            \item It has $m$ inputs,
            \item 1 output and
            \item uses a ($\log_2(m)$)-bit control signal input to choose
                which input is connected to the output.
        \end{itemize}
    \item \textbf{Demultiplexer}
        \begin{itemize}
            \item It has 1 input,
            \item $m$ outputs and
            \item uses a ($\log_2(m)$)-bit control signal to choose which
                output is connected to the input
        \end{itemize}
\end{enumerate}

Remember that the inputs and outputs are $n$ bits, but they obviously have
to match up. The connection that is made is continuous, because both
components are \textit{combinatorial}.

\subsubsection{Multiplexer}%
\label{ssub:Multiplexer}
The behaviour of a 2-input, 1-bit multiplexer is as follows:
\begin{equation*}
    r = (\neg \ c \wedge x) \vee (c \wedge y)
\end{equation*}

It can be more clearly represented by this table:
\begin{center}
    \begin{tabular}{|c c c|c|}
        \hline
        c & x & y & r \\
        \hline
        0 & 0 & ? & 0 \\ \hline
        0 & 1 & ? & 1 \\ \hline
        1 & ? & 0 & 0 \\ \hline
        1 & ? & 1 & 1 \\ \hline
    \end{tabular}
\end{center}


\subsubsection{Demultiplexer}%
\label{ssub:Demultiplexer}

The behaviour of a 2-output, 1-bit demultiplexer is as follows:
\begin{align*}
    r_0 &= \neg \ c \wedge x \\
    r_1 &- c \wedge x
\end{align*}

It can be more clearly represented in this table:
\begin{center}
    \begin{tabular}{|c c|c c|}
        \hline
        $c$ & $x$ & $r_1$ & $r_0$ \\ \hline
        0 & 0 & ? & 0 \\ \hline
        0 & 1 & ? & 1 \\ \hline
        1 & 0 & 0 & ? \\ \hline
        1 & 1 & 1 & ? \\ \hline
    \end{tabular}
\end{center}

Multiplexers can be used to for isolated replication. If we line up 4
multiplexers, we get a control signal in to choose between two choices,
and we get one of those out, meaning that we end up with a 4-bit output.
We can scale this in the same way as before and use the \textit{cascaded}
pattern, meaning that they interact with one another. We now need a 2-bit
control signal however, because in the other design, each multiplexer uses
the same control signal.

Building on this idea, we can look at two more components:
\begin{enumerate}
    \item Half adder
        \begin{itemize}
            \item Has 2 inputs, $x$ and $y$,
            \item Computes the 2-bit result $x + y$
            \item Has 2 outputs: a sum $s$ and a carry out $co$ (which are
                the least significant bit and the most significant bit of
                the result)
        \end{itemize}
    \item Full adder
        \begin{itemize}
            \item Has 3 inputs: $x$, $y$, and a carry-in $ci$
            \item Computes the 2-bit result $x + y + ci$
            \item Has 2 outputs: a sum $s$ and a carry out $co$ (which are
                the least significant bit and the most significant bit of
                the result)
        \end{itemize}
\end{enumerate}

\subsection{Half adder}%
\label{sub:half}
The behaviour of the half adder looks like this:
\begin{center}
    \begin{tabular}{|c c|c|c|}
        \hline
        $x$ & $y$ & $co$ & $s$ \\ \hline
    \end{tabular}
\end{center}

And can be displayed by the following equations:
\begin{align*}
    co &= x \wedge y \\
    s &= x \oplus y
\end{align*}

\subsection{Full adder}%
\label{sub:full}
The behaviour of a full adder looks like this:
\begin{center}
    \begin{tabular}{|c c c|c|c|}
        \hline
        $ci$ & $x$ & y & $co$ & $s$ \\ \hline
        0 & 0 & 0 & 0 & 0 \\ \hline
        0 & 0 & 1 & 0 & 1 \\ \hline
        0 & 1 & 0 & 0 & 1 \\ \hline
        0 & 1 & 1 & 1 & 0 \\ \hline
        1 & 0 & 0 & 0 & 1 \\ \hline
        1 & 0 & 1 & 1 & 0 \\ \hline
        1 & 1 & 0 & 1 & 0 \\ \hline
        1 & 1 & 1 & 1 & 1 \\ \hline
    \end{tabular}
\end{center}

And can be displayed by the following equations:
\begin{align*}
    co &= (x \wedge y) \vee (x \wedge ci) \vee (y \wedge ci) \\
       &= (x \wedge y) \vee ((x \oplus y) \wedge ci) \\
    s &= x \oplus y \oplus ci
\end{align*}

A full adder is kind of like two half adders put together in a nice way.

There is a circuit for the full adder that allows us to implement the
cascading design choice to replicate the $n$-bit addition; cascading
because of the carries that we have are \textit{cascaded} into one
another. It might not be immediately clear that it's cascaded when we
initially looked at it, but now as we look at it from a higher value, then
we can clearly see that it is indeed cascaded.


Moving onto equality comparators, we have 2 final building blocks:
\begin{enumerate}
    \item An equality comparator
        \begin{itemize}
            \item Has 2 inputs: $x$ and $y$,
            \item computes the 1 output as:
                \begin{equation*}
                    r = \bigg \{
                   \begin{matrix}
                       1 & \text{if } $x = y$ \\
                       0 & \text{otherwise}
                   \end{matrix}
                \end{equation*}
        \end{itemize}
    \item A less-than comparator
        \begin{itemize}
            \item Has 2 inputs: $x$ and $y$,
            \item computes the 1 output as:
                \begin{equation*}
                    r = \bigg \{
                        \begin{matrix}
                            1 & \text{if } x < y \\
                            0 & \text{otherwise}
                        \end{matrix}
                \end{equation*}
        \end{itemize}
\end{enumerate}

All of the inputs and outputs are only 1 bit.

Even though we only have these two, we can do a lot of other comparators,
for example, we can get $x \neq y$ from passing the result of an equality
gate through a not gate.

\subsection{Equality comparator}%
\label{sub:equality}
The behaviour of the equality comparator is as follows:
\begin{equation*}
    r = \neg (x \oplus y)
\end{equation*}

And can be modelled by this table:
\begin{center}
    \begin{tabular}{|c c|c|}
        \hline
        $x$ & $y$ & $r$ \\ \hline
        0 & 0 & 1 \\ \hline
        0 & 1 & 0 \\ \hline
        1 & 0 & 0 \\ \hline
        1 & 1 & 1 \\ \hline
    \end{tabular}
\end{center}

\subsection{Less-than comparator}%
\label{sub:less-than}

The behaviour of the less-than comparator can be modelled as follows:
\begin{equation*}
    r = \neg x \wedge y
\end{equation*}

And can be shown in this table:
\begin{center}
    \begin{tabular}{|c c|c|}
        \hline
        $x$ & $y$ & $r$ \\ \hline
        0 & 0 & 0 \\ \hline
        0 & 1 & 1 \\ \hline
        1 & 0 & 0 \\ \hline
        1 & 1 & 0 \\ \hline
    \end{tabular}
\end{center}

In the same way that we had a ripple carry adder, we can have an $n$-bit
comparison. For a 3-bit number, for them to be equal, every single bit
must match. Therefore, if we first compare the first bits, and then and
this with the second bits, and then and it again this with the third bits,
this will give us the result we require.

For a cyclic less-than format, we have to do use a less than and an
equality block to be able to go through and work out which is bigger.
Let's say we take two numbers, $x$ and $y$, where $x = 123$ and $y = 223$.
We can see that $y$ is bigger than $x$, because we started at the left
most digit and compared these. This is quite an easy example, so let's
look at another one.

In this example, $x = 121, \ y = 123$. Here, we start at the left most
digit, which are the same. So, we move inward, which is the same again.
Doing this one more time, we can see that this digit of $y$ is bigger than
that of $x$, therefore $y > x$. We have some rules here then:
\begin{enumerate}
    \item If $x_i < y_i$, then $x < y$
    \item If $x_i > y_i$, then $x \not < y$
    \item if $x_i = y_i$, then $x < y$ IF the $rest(x) < rest(y)$
\end{enumerate}

In the format of an equation:
\begin{align*}
    x<y \text{if } (x_i < y_i) \vee (x_i = y_i \wedge \text{ rest } x <
    \text{ rest } y)
\end{align*}

We have this joined up to each bit of the input.

The final set of components are the \textbf{encoder} and the
\textbf{decoder}. They can also be viewed as translators, or, formally:
\begin{enumerate}
    \item an $n$-to$m$ encoder translates an $n$-bit input into some
        $m$-bit code word, and
    \item an $m$-to-$n$ decoder translates an $m$-bit code word
        \textbf{back} into the same $n$-bit output.
\end{enumerate}

Where if only one output is allowed to be 1 at a time, we call it a
\textbf{one of many} encoder.

A \textit{general} building block is impossible because it depends on the
scheme for encoding or decoding. Let's look at an example:
\begin{enumerate}
    \item To encode, take $n$ inputs, for example $x_i$ for $0 \le i < n$
        and produce an unsigned integer $x'$ that determines which $x_i =
        1$
    \item To decode, take $x'$ and set the right $x_i = 1$
\end{enumerate}

Where for all $j \not = i, x_j' = 0$ (so both are \textbf{one-of-many})
and this means we recover the original $x$.

We can map the behaviour by creating a truth table:
\begin{center}
    \begin{tabular}{|c c c c|c c|}
        \hline
        $x_3$& $x_2$ & $x_1$ & $x_0$ & $x_1'$ & $x_0'$ \\
        \hline
        0 & 0 & 0 & 1 & 0 & 0 \\
        \hline
        0 & 0 & 1 & 0 & 0 & 1 \\
        \hline
        0 & 1 & 0 & 0 & 1 & 0 \\
        \hline
        1 & 0 & 0 & 0 & 1 & 1 \\
        \hline
    \end{tabular}
\end{center}
Which also correlates to:
\begin{align*}
    x_0' = x_1 \vee x_3 \\
    x_1' = x_2 \vee x_3
\end{align*}

The decoder should be exactly the opposite of this, and can be modelled by
this equation:
\begin{align*}
    x_0 = \neg x'_0 \wedge \neg x_1' \\
    x_1 = x_0' \wedge \neg x_1' \\
    x_2 = \neg x_0' \wedge x_1' \\
    x_3 = x_0' \wedge x_1'
\end{align*}

There is a problem with this because if we break the rules and set more
than one bit of $x$ to 1, then the encoder fails because it produces two
1s. The solution is to produce a priority encoder, where one input is
given priority over another. 
\begin{center}
    \begin{tabular}{|c c c c|c c|}
        \hline
        $x_3$& $x_2$ & $x_1$ & $x_0$ & $x_1'$ & $x_0'$ \\
        \hline
        0 & 0 & 0 & 1 & 0 & 0 \\
        \hline
        0 & 0 & 1 & ? & 0 & 1 \\
        \hline
        0 & 1 & ? & ? & 1 & 0 \\
        \hline
        1 & ? & ? & ? & 1 & 1 \\
        \hline
    \end{tabular}
\end{center}

\section{Sequential logic}%
\label{sec:sequential}

Imagine that we need a cyclic $n$-bit counter, such as a component whose
output steps through the values:
\begin{equation*}
    0,1,\cdots,2^n -1, 0,1,\cdots
\end{equation*}

But is otherwise free running.

We already know how to make an $n$ bit adder, so we can just compute $r
\leftarrow r + 1$ over and over. It sounds cool, but we can't initialise the
value, and also we don't let the output of each full-adder settle before
it's used again as an output. 

The main issue that we have is that combinatorial logic has some
limitations because we can't control \textit{when} a design computes some
output, nor remember the output when produced. A good solution is
\textbf{sequential logic} which demands that there is some way to control
components, one or more components that remember the state that they're
in, and a mechanism to perform computation as a sequence of steps, rather
than continuously.

\subsection{Clocks}%
\label{sub:Clocks}

A clock is a signal that alternates between 1 and 0. We call the period of
time where it is 1 as \textit{positive level}, and when it has a value of
0, it has a \textit{negative level}. Also, when it changes from positive
to negative, we call this a \textbf{negative edge}, while the opposite of
this is called a \textbf{positive edge}.

We want to clock to \textit{trigger} events in order to synchronise
components within a design, while the clock frequency is how many clock
cycles happen in a unit of time. It's gotta be fast enough that the design
goals are met, but slow enough that the critical path of a given step is
not exceeded. The faster the clock ticks, the better things are, but we
can't go too fast. The clock is therefore a conductor.

An $n$ phase-clock is distributed as $n$ separate signals along $n$
separate wires. A 2-phase instance is really useful because it has the
features of a 1-phase clock, such as the clock period etc, but there is a
guarantee that positive levels of the two clocks don't overlap, and the
behaviour is parameterisable by altering the clock length ($\delta_i$).

\subsection{Latches and flipflops}%
\label{sub:Latches}

A bistable component can exist in two stable states, like 0 or 1 at a
given point. It can:
\begin{itemize}
    \item Retain some \textbf{current state} -- Q --, which can also be read as
        output, and
    \item be updated to some \textbf{next state} -- Q' -- which is provided as an
        input.
\end{itemize}

Under control of an enable signal \textit{en}. We say it is:
\begin{itemize}
    \item \textbf{Level-triggered} and hence a latch if it's updated by a
        given level of \textit{en}, or
    \item \textbf{Edge triggered} and hence a flipflop if it's updated by
        a given edge of \textit{en}.
\end{itemize}

They both have different properties, but both are useful.

\subsubsection{SR component}%
\label{ssub:SR}
An "SR" latch/flip-flop component has two inputs: S (for set), and R (for
reset). When enabled, the following behaviour can be observed:
\begin{itemize}
    \item S = 0, R = 0, the component retains Q
    \item S = 1, R = 0, the components updates to Q = 1,
    \item S = 0, R = 1, the component updates to Q = 0,
    \item S = 1, R = 1, the component is metastable.
\end{itemize}

When it's not enabled, the component is in `storage mode'

It can also be described by the truth table:
\begin{center}
    \begin{tabular}{|c c|c c|c c|}
        \hline
        $S$ & $R$ & $Q$ & $\neg Q$ & $Q'$ & $\neg$ $Q'$ \\ \hline
        0 & 0 & 0 & 1 & 0 & 1 \\ \hline
        0 & 0 & 1 & 0 & 1 & 0 \\ \hline
        0 & 1 & ? & ? & 0 & 1 \\ \hline
        1 & 0 & ? & ? & 1 & 0 \\ \hline
        1 & 1 & ? & ? & ? & ? \\ \hline
    \end{tabular}
\end{center}

\subsubsection{D component}%
\label{ssub:D}
A "D" latch/flip-flop component has a single input - $D$. When it's
enabled, the following behaviour can be observed:
\begin{itemize}
    \item $D$ = 1, the component updates to $Q$ = 1
    \item $D$ = 0, the component updates to $Q$ = 0
\end{itemize}

When it's not enabled, the component is in storage mode, and retains $Q$.
The behaviour can be modelled by this truth table:
\begin{center}
    \begin{tabular}{|c|c c|c c|}
        \hline
        $D$ & $Q$ & $\neg Q$ & $Q'$ & $\neg Q'$ \\ \hline
        0 & ? & ? & 0 & 1 \\ \hline
        1 & ? & ? & 1 & 0 \\ \hline
    \end{tabular}
\end{center}

How do we design a simple SR latch? We use two \textit{cross-coupled} NOR
gates. It can be shown through these equations:
\begin{align*}
    S \bar \wedge Q = \neg Q \\
    R \bar \wedge \neg Q = Q
\end{align*}

There's a loop here, because in order to know the output from the top NOR
gate, we need to know the output from the bottom one, and vice versa. So
what gives?? Well, it's alright if we look at the behaviour of a NOR gate.

We know that a NOR gate gives us a 1 only when both inputs are 0. If
either input is 1, then NOR will produce a 0. This is really important for
our gate to actually function.

If we set $S$ = 0, then the output from the top NOR gate is forced to
produce 0. Then, if $R$ is 0, then the output is 1. Similarly, if $R$ is 0,
the output for THAT NOR gate is 1. Therefore, we get what we want. The
reverse is true when $R$ is 1 and $S$ is 0.

There is some form of issue, though, because if both $R$ and $S$ are 0,
then either the top NOR gate spits out 1, or the bottom NOR gate does.
That's just the way it is. There's no logic error, it's just a bit weird.

There's just one case left to study. If $S$ and $R$ are 1. In the truth
table, we stated that we didn't care what happened, but in the
implementation, then both $Q$ and $\neg Q$ are both 0, which is clearly a
bloody issue because $Q$ cannot equal $\neg Q$. What happens now? Well,
the latch has to settle in some case, either $Q = 1$ or $Q = 0$. We have 0
control over what happens. We don't want this behaviour at all. This is
what meta stability means. It's stable in some sense, but kind of not.

\subsubsection{Updates}%
\label{ssub:Updates}
We want to be able to control when updates occur, because without this,
it's not really a latch yet. To do this, we have an AND gate before we get
to the criss-cross NOR gated latch, so we AND S and the enable signal, and
R and the enable signal, and then pass the results from this into the old
S and R respectively. This is a technique known as \textbf{gating}.

Ok, this just in. WE CAN GET RID OF THE ISSUE OF META-STABILITY. Yep, you
heard that right. We force $R = \neg S$, so we get either $S = 0, R = 1$
or $S = 1, R = 0$. Dope, right?

The difference between a latch and a flip-flop is that is has a little
triangle on the symbol, indicating that it's edge triggered.

For a latch, the point value of $Q$ only changes when the enable signal is
1, it then matches the input $D$. Some people call this a `transparent'
latch, because it's like the latch isn't even there.

In a flip flop, the input only changes to the input $D$ on the edges of
the clock (when the enable switch is on).

\subsubsection{Registers}%
\label{ssub:Registers}
We normally group these components into \textbf{registers}, which is an
$n$ bit register that can then store an $n$ bit value. We take each latch
as a single bit.

\subsection{Returning to the original issue}%
\label{sub:original}
So we have some data path of computation and/or storage components, and a
control path, that tells the components in the data path what to do and
when to do it.

What we have is some latch based register, that takes in $\phi_1$, the
first clock signal as an enable signal. This gives an input to some
combinatorial logic, which then passes the result into another latch based
register that takes in $\phi_2$, the second clock signal. This register
passes the information back to the first register. And so on. This creates
a kind of buffer that gives us a counter that we wanted. The two registers
mean that the loop is broken, because it is impossible for both registers
to be enabled at the same time; breaking the infinite loop that we would
have without them.

The combinatorial logic in the middle is just a big boy ripple carry
adder. There is also a reset signal that is NOTED, and then ANDED with the
individual bits of the ripple carry adder, meaning that when the RESET
signal is 1, it becomes 0, thereby resetting all bits, because $x \wedge 0
= 0$.

\subsection{Memory Component}%
\label{sub:memory}

An $n$-bit register based on latches or flip-flops has a couple of
limitations:
\begin{enumerate}
    \item Each latch or flip-flop in the register needs a fairly big
        number of transistors. This limits the viable capacity
    \item The register is also not addressable, because an address or
        index allows dynamic rather than static reference to some stored
        data. That means that we have something like variables, when we
        want an array.
\end{enumerate}

The solution to these limitations is to have a memory component. The
memory is connected to a \textbf{user} which could be a CPU by control
signals, a data bus, and an address bus. Sound familiar? We'll also say
that the memory has a capacity of $n = 2^{n'}$ addressable words and each
word is $w$ bits, where $n \gg w$.

There are a number of ways to classify a given memory component:
\begin{enumerate}
    \item Volatility
    \begin{itemize}
        \item \textbf{Volatile} means that the content is lost when the
            component is powered off
        \item \textbf{Non volatile} means that the content is not lost
            when the component is powered off
    \end{itemize}
    \item Interface type:
    \begin{itemize}
        \item \textbf{Synchronous} where a clock or pre-determined timing
            information synchronises steps
        \item \textbf{Asynchronous} where a protocol synchronises steps
    \end{itemize}
    \item Access type
    \begin{itemize}
        \item \textbf{Random vs. Constrained} access to content.
        \item \textbf{Random Access Memory (RAM)} which can be read from
            \textit{and} written to.
        \item \textbf{Read Only Memory (ROM)} which can only be read to
    \end{itemize}
\end{enumerate}

We're going to focus on a \textbf{volatile, synchronous RAM}.

\subsubsection{History of memory}%
\label{ssub:history-mem}

In the olden days, we had \textbf{delay line} memory, that contained
mercury. There is a speaker at one end to store sound waves into the line
and a microphone at the other end to read them out.

Values are stored in the sense that the corresponding waves take some time
to propagate, when they get to one end, they are either replaced, or fed
back into the other.

This is known as sequential access because you need to wait for the data
you want to appear.

After delay line came \textbf{magnetic-core} memory, where the basic idea
is that the memory is a matrix of small magnetic rings or cores which can
be magnetically polarised to store values. Wires are threaded through the
cores to control them (so to read or write values). The magnetic
polarisation is retained, so this is non-volatile. Sometimes, main memory
is termed \textbf{core-memory} (or \textbf{core dump}), which is in
reference to this.

\subsubsection{Low-level implementation}%
\label{ssub:Low-level implementation}

There are two kinds of RAM that we can use: \textbf{Static RAM} or (SRAM)
and \textbf{Dynamic RAM} or (DRAM).

SRAM is:
\begin{itemize}
    \item manufacturable in lower densities 
    \item More expensive to manufacture
    \item Faster to access
    \item Easier to interface with
    \item Ideal for latency optimised contexts (like cache memory)
\end{itemize}

DRAM is:
\begin{itemize}
    \item Manufacturable in higher densities.
    \item Less expensive to manufacture
    \item Slower to access
    \item Harder to interface with
    \item Ideal for capacity optimised contexts (like main memory)
\end{itemize}

Simply, an \textbf{SRAM cell} resembles two NOT gates on the inner loop.
It basically carries around a signal that is reinforced by the NOT gates.
On the outside, there are two transistors that are connected to some
control signal $wl$. The transistors allow access to the values inside the
loop. To read from this, we pre-charge the transistors to 1. This is
logically inconsistent, but it means that we allow one of them to be
overridden after we allow access to the inside of the loop (through $wl$).
To write to this, we charge the left transistor (or $bl$) to the value,
and the right one to the opposite (since it is called $\neg bl$).

An issue that we might find is different signal strengths. We're going to
kind of brush it under the carpet and not think about it.

In total, we'd need 6 transistors: 2 for each of the 2 NOT gates, and one
each for the two on the outside. Therefore, it's known as a \textbf{6t}
SRAM cell. It's loads less than the 20 or so required by the latches and
flip-flops. This therefore solves one of the limitations of the latches.

A \textbf{DRAM} cell is constructed only using 1 single transistor and a
capacitor (kind of like a battery because it stores charge). The
transistor again has an enable signal $wl$, and some input $bl$. If we
want to read, then we set $bl$ to 1. If the contents is 1, then there is a
discharge through $bl$, else there will not be.  To be able to write, we
set $bl$ to the required value, and then set $wl$ to 1. After this, the
capacitor is charged with the required value.

The issue with this cell is that the capacitor can only do limited
charges. Also, the same issue as before, because the signal strength might
not be enough for the required circuit. Additionally, the charge stored in
the capacitor will decay over time. We need some refresh thing in the
background to ensure that the value is not lost over time.  Reading from a
DRAM cell means that the value is destroyed, so we need to rectify that
through the refresh mechanism. The latency is long from a DRAM cell
because the capacitor has a long access time.

\subsubsection{Higher level implementation}%
\label{ssub:high-level}
A \textbf{memory device} is constructed from (roughly) three components:
\begin{enumerate}
    \item A \textbf{memory array} (or matrix) of replicated cells with
        \textit{r} rows and \textit{c} columns.
    \item A \textbf{row decoder} which given an address (de)activates
        associated cells in that row, and
    \item A \textbf{column decoder} which given an address (de)selects
        associated cells in that column
\end{enumerate}

Along with additional logic to allow use:
\begin{enumerate}
    \item \textbf{Bit line conditioning} to ensure that the bit lines are
        strong enough to be effective,
    \item \textbf{Sense amplifiers} to ensure that the output from the
        array is usable.
\end{enumerate}

To access these bad boys, we get an input to the row decode from the
address bus, and an output from the column decode. It's kind of like a
multiplexer and then a demultiplexer.

The difference between DRAM and SRAM is simply that the cells in the
middle are DRAM rather than SRAM.

DRAM also has buffers (both for the row and the column). 

\section{Finite State Machines}%
\label{sec:FSMs}

\subsection{Automata Theory}%
\label{sub:automata}

First things first, we need some definitions:
\begin{itemize}
    \item \textbf{Alphabet}: a non-empty set of symbols
    \item \textbf{String}: with respect to some alphabet $\Sigma$ is a
        sequence of finite length, whose elements are members of $\Sigma$.
    \item \textbf{Language}: a set of strings.
\end{itemize}

Finite state machines are a model of computation. What does this mean?
It's a set of allowable (or possible) operations. It is basically an
idealised computer that is in some finite set of states at a given point
in time. It accepts an input string (with respect to some alphabet) one
symbol at a time. Each symbol produces a change in state. Once it's run
out of inputs, the computer stops. Depending on the state it stops in, it
can either \textit{accept} or \textit{reject} the string. For a language
of all possible strings that the computer could accept, the
computer \textbf{accepts} the language.

Finite state machines do not have any memory. The more advanced machine
that you have, the more memory you have. We're looking at a kind of
mid-rate deal, that does not have any memory.

For a more formal definition of FSMs, it is a tuple:
\begin{equation*}
    C = (S,s,A,\Sigma,\Gamma,\delta,\omega)
\end{equation*}

With the following definitions:
\begin{itemize}
    \item $S$ is a finite set of states that includes a \textbf{start
        state}, $s \in S$
    \item $A \subseteq S$, a finite set of \textbf{accepting states}
    \item An \textbf{input alphabet} $\Sigma$ and an \textbf{output
        alphabet} $\Gamma$
    \item A \textbf{transition function}:
        \begin{equation*}
            \delta : S \times \Sigma \rightarrow S
        \end{equation*}
    \item An \textbf{output function}:
        \begin{equation*}
            \omega : S \rightarrow \Gamma
        \end{equation*}
        In the case of a \textbf{Moore FSM}, or
        \begin{equation*}
            \delta : S \times \Sigma \rightarrow \Gamma
        \end{equation*}
        In the case of a \textbf{Mealy FSM} 
\end{itemize}

An empty input is called $\epsilon$

We need to design an FSM that decides whether a binary sequence $X$ has an
odd number of 1 elements in it. The \textit{accepting state} is
$S_{\text{odd}}$.

For example, say we have the input string $X = \langle 1,0,1,1 \rangle$
the transition are:
\begin{equation*}
    \rightarrow S_{even} \xrightarrow{X_0 = 1} S_{odd} \xrightarrow{X_1 =
0} S_{odd} \xrightarrow{X_2 = 1} S_{even} \xrightarrow{X_3 = 1}
    S_{odd}
\end{equation*}

So the input is accepted because there are an odd number of 1 elements.

The real world example of this is to use a regular expression (or Regex).

\subsection{FSMs in hardware}%
\label{sub:hardware-fsm}

Using an FSM with a latch based implementation is that:
\begin{enumerate}
    \item The state is retained in a register
    \item $\delta$ and $\omega$ are simple combinatorial logic
    \item Within the current clock cycle:
        \begin{enumerate}
            \item $\omega$ computes the output from the current state and
                input
            \item $\delta$ computes the next state from the current state
                and input
        \end{enumerate}
    \item The next state is latched by an appropriate feature in the
        clock.
\end{enumerate}

This is a computer that we can actually build now!

We have two options as to the choice of encoding the state of the machine:

There is \textbf{binary encoding}:
\begin{align*}
    S_0 \mapsto \langle 0, 0, 0 \rangle \\
    S_1 \mapsto \langle 1, 0, 0 \rangle \\
    S_2 \mapsto \langle 0, 1, 0 \rangle \\
    S_3 \mapsto \langle 1, 1, 0 \rangle \\
    S_4 \mapsto \langle 0, 0, 1 \rangle \\
    S_5 \mapsto \langle 1, 0, 1 \rangle 
\end{align*}

Or, there is \textbf{One- hot encoding}:
\begin{align*}
    S_0 \mapsto \langle 1, 0, 0, 0, 0, 0 \rangle \\
    S_1 \mapsto \langle 0, 1, 0, 0, 0, 0 \rangle \\
    S_2 \mapsto \langle 0, 0, 1, 0, 0, 0 \rangle \\
    S_3 \mapsto \langle 0, 0, 0, 1, 0, 0 \rangle \\
    S_4 \mapsto \langle 0, 0, 0, 0, 1, 0 \rangle 
    S_5 \mapsto \langle 0, 0, 0, 0, 0, 1 \rangle 
\end{align*}

There is a trade off here because there is more space required for the
one-hot, but there is less energy required because we are only flipping
two bits at a time.

Now, we want to design an FSM that acts like a cyclic counter modulo $n$
(rather than $2^n$ as before). If $n = 6$ for example, we want a component
whose output $r$ steps through values:
\begin{equation*}
    0, 1, 2, 3, 4, 5, 0, 1, \cdots
\end{equation*}

The first couple of steps of our algorithm state that we need to
enumerate each state, and then give them some abstract label. We can
represent this using a table:

\begin{center}
    \begin{tabular}{|c|c|c|}
        \hline
        & $\delta$ & $\omega$ \\ \hline
        $Q$ & $Q'$ & $r$ \\ \hline
        $S_0$ & $S_1$ & 0 \\ \hline
        $S_1$ & $S_2$ & 1 \\ \hline
        $S_2$ & $S_3$ & 2 \\ \hline
        $S_3$ & $S_4$ & 3 \\ \hline
        $S_4$ & $S_5$ & 4 \\ \hline
        $S_5$ & $S_0$ & 5 \\ \hline
    \end{tabular}
\end{center}

Next, we need to design the state assignment step:
\begin{equation*}
    S_i \mapsto i (0 \le i \le 5)
\end{equation*}

Since $2^3 = 8 > 6$, we can represent them using 6 concrete values,
namely:
\begin{align*}
    S_0 \mapsto \langle 0, 0, 0 \rangle = 000_{2}\\
    S_1 \mapsto \langle 1, 0, 0 \rangle = 001_{2} \\
    S_2 \mapsto \langle 0, 1, 0 \rangle = 010_{2} \\
    S_3 \mapsto \langle 1, 1, 0 \rangle = 011_{2} \\
    S_4 \mapsto \langle 0, 0, 1 \rangle = 100_{2} \\
    S_5 \mapsto \langle 1, 0, 1 \rangle = 101_{2} 
\end{align*}

And capture:
\begin{align*}
    &Q = \langle Q_0, Q_1, Q_2 \equiv \text{the current state} \\
    &Q' = \langle Q'_0, Q'_1, Q'_2 \rangle \equiv \text{the next state}
\end{align*}
in a 3-bit register (so like 3 latches or 3 flip-flops)

We can rewrite the abstract labels to give us some concrete truth table:
\begin{center}
    \begin{tabular}{|c c c|c c c|c c c|}
        \hline
        $Q_2$ & $Q_1$ & $Q_0$ & $Q'_2$ & $Q'_1$ & $Q'_0$ & $r_2$ & $r_1$ & $r_0$ \\ \hline
        0 & 0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 \\ \hline
        0 & 0 & 1 & 0 & 1 & 0 & 0 & 0 & 1 \\ \hline
        0 & 1 & 0 & 0 & 1 & 1 & 0 & 1 & 0 \\ \hline
        0 & 1 & 1 & 1 & 0 & 0 & 0 & 1 & 1 \\ \hline
        1 & 0 & 0 & 1 & 0 & 1 & 1 & 0 & 0 \\ \hline
        1 & 0 & 1 & 0 & 0 & 0 & 1 & 0 & 1 \\ \hline
        1 & 1 & 0 & ? & ? & ? & ? & ? & ? \\ \hline
        1 & 1 & 1 & ? & ? & ? & ? & ? & ? \\ \hline
    \end{tabular}
\end{center}

The big block of `don't care' means that we should never really reach
those. We can also apply the technique of Karnaugh maps.

And that's it for the Dan Page bit of the course!

\section{Data, control, and instructions}%
\label{sec:Data}

Abstraction is pretty great. A higher level only needs to know how to
interface with the level directly below it. So, why should we care about
hardware? Well, it's kind of important because if we write a program
that's too slow, or it isn't running right, having a grasp of how a
computer works lower down the levels of abstraction will really help us
to be able to fix whatever problems might arise.

A processor is dictated by two separate functions:
\begin{itemize}
    \item \textbf{Control}: the information and instructions
    \item \textbf{Data}: the information operated on to get the result.
\end{itemize}

These two influences form two paths into the processor logic.

The definition of \textbf{data} is simply some stored information. It can
be stored or formatted in a number of ways. Data stores information and
forms input to, and outputs from, calculations.

Data is stored in storage elements. There are many different storage
elements in modern processing systems. During this course, we only need to
focus on two: \textbf{memory} and \textbf{registers}. For this section, we
can treat them both as \textit{black boxes}, meaning that we don't really
care what's going on inside. Processor instructions always operate on data
in registers, and if they're lucky, they get to operate on data in memory,
too.

\textbf{Control} is also information, but the usage differs slightly from
data. It specifies what needs to be done and is applied to a system,
rather than consumed by it.

\subsection{Instructions}%
\label{sub:Instructions}
Instructions are really useful for telling the computer exactly what we
want to be doing. For example, if an instruction ``A" is called, then the
processor should do ``X". At a high level, we can treat them as abstract
symbols, whose value causes different processor behaviour. Instructions
need to be \textbf{decoded} before they can be used.

An instruction is \textit{also} information. However, it has a defined
purpose, which is to specify the exact amount of work that needs to be
done by a processor. This leads it to have a specific form and formatting.
Only a subset of all possible control values are actually valid
instructions.

Instructions allow the encoding of control information that is necessary
to control a computing system. The trick is to use a unique code (called
the \textbf{op-code}) to signify a unique function.

There are many different possible encodings of instructions for the same
meaning. Each system has its own tailored decode module to figure out the
meaning of the op-codes to generate control signals.

There is more than one way to make a decoder. It could be combinatorial
logic, demultiplexer based, or lookup based.

\section{Memory}%
\label{sec:Memory}

Memory, simply, is a place to store information. There are two basic
operations that we can perform on memory: \textbf{read} and
\textbf{write}. Each piece of information in memory is assigned to a
unique address. In order to access or update information, we need to
specify this address to a memory, and then our information can be returned
or changed. Addresses are specified as indexes. Values can be op-codes.

A single memory location can combine both an instruction and some data.
This can be useful for constant based operations. Consider
\lstinline|ADD1| or something that loads a constant, such as
\lstinline|MOVE2|. We could express \lstinline|ADD| as 1, therefore we
could also express \lstinline|ADD1| as \lstinline|11|. There is a memory
hierarchy that looks like this:
\begin{enumerate}
    \item Register
        \begin{itemize}
            \item 32 words
            \item Access time of $<1$ ns
        \end{itemize}
        \item L1 cache
            \begin{itemize}
                \item ~32KB
                \item ~1ns access time
                \item Part of SRAM
            \end{itemize}
        \item L2 cache
        \begin{itemize}
            \item ~512KB
            \item 5-10ns access time
            \item Part of SRAM
        \end{itemize}
        \item L3 cache
        \begin{itemize}
            \item 1-8MB
            \item 10-100ns access time
            \item Part of SRAM
        \end{itemize}
    \item Main memory (DRAM)
    \begin{itemize}
        \item 1-16GB
        \item ~100ns access time
    \end{itemize}
    \item Hard disk/SSD
        \begin{itemize}
            \item 100-1000GBs
            \item ~10ms access time
        \end{itemize}
\end{enumerate}

\subsection{Addressing}%
\label{sub:Addressing}
Addressing is when we want to access memory, we need to specify which
memory address to use. For example: \lstinline|MEM[10]| means to access
memory address 10. Ideally, we could specify a memory address directly
every time, but that's not always possible. Sometimes, we want to specify
a sequence of addresses. Therefore, we have invented a load of different
ways to specify a memory address.

\subsubsection{Immediate addressing}%
\label{ssub:immediate}
Immediate addressing is when data is supplied \textbf{in an instruction}.
There is no real memory address and all information is embedded in the
instruction. Also, data is immediately available. It's really fast and
simple (the simplest). An example looks like: \lstinline|rl <- 42|.

All of the information is embedded in instruction, so it's predictable.
This makes it really fast. It's pretty easy to understand and it's good
for optimisers to analyse. Unfortunately, in the words of the mighty Dan
Page, there's no free lunch. There is a lack of flexibility and it's gotta
be inserted statically. There's a limited range of instructions (seeing as
it's limited by the permitted number of operand bits in the opcode).

\subsubsection{Direct addressing}%
\label{ssub:direct}
An instruction like: \lstinline|MEM[10]| is pretty cool, but how is it
formed? It's formed in the kind of format: Operation | Operand1 |
Operand2. For example \lstinline|6, 10, 42|. The exact memory address used
is embedded in the instruction. This is known as \textbf{direct
addressing}. The exact memory address used is embedded in the instruction.

Direct addressing has the same pros and cons as Immediate
addressing~\ref{ssub:immediate}. But, it's a little slower in return for
a larger range.

\subsubsection{Memory-indirect addressing}%
\label{ssub:memory-indirect}

Memory-indirect addressing solves the problem of limited range by storing
the address to be accessed in memory itself. 

An example would be: MEM[MEM[42]] which means to go and look at the memory
address in 42 and fetch the value. That value is the address to write the
value in r1 to.

It's good because it's got a larger range and the source memory location
for the address may be dynamically changed.

Unfortunately, there are some bad points. The first memory address is
still statically compiled. The range restriction is just changed to the
initial memory range. It's also slower than direct addressing.

\subsubsection{Register-Indirect addressing}%
\label{ssub:register-indirect}

This method provides even more flexibility. It uses the register's value
as the memory address: MEM[r1] <- r1.

There are loads of advantages to register indirect addressing like the
memory address can be dynamically computed and the value does not need to
be stored in the instruction thereby reducing code size. The register is
internal to the processor so it's faster and more energy efficient.

This also allows for native support of pointers. Accessing indirectly is
equivalent to a dereferencing operation, like *p in C.

\subsubsection{Indexed addressing}%
\label{ssub:indexed}
Sometimes, you just gotta define a base address and access memory based on
this. It's pretty useful for stacks, arrays, and caches etc. Indexed
addressing extends indirect addressing to support this. We have a
\textbf{base} and an \textit{offset}.

Normally, the base and the offset are both stored in the registers, but
this doesn't have to be the case. We get instructions like: MEM[r1 + r2]
<- r3. Here, r1 is the base and r2 is the offset. Base and offset can be
varied independently.

Many implementations support the base and offset construct natively.
Architectures often have a dedicated register to help, normally called
something like the stack pointer or the base register.

The stack/base registers may or may not be general purpose depending on
the architecture. The offset usually comes from an additional general
purpose register. An example of indexed addressing based on an array. 


\section{Compilers}%
\label{sec:Compilers}
We're going to be looking at an 8 bit machine and how we'd go about
building a machine. Anytime you start with producing a computer or
processor, we need to make an instruction set. How do we design an
instruction set while keeping it simple?

\subsection{Instruction set}%
\label{sub:Instruction set}
Firstly, in any machine, we need some \textit{registers}. We need one or
two things to hold the arithmetic logic. The registers we have are:
\begin{itemize}
    \item \textbf{PC} - The program counter
    \item \textbf{AREG} - Holds the arithmetic stuff
    \item \textbf{BREG} - Same as AREG
    \item \textbf{OREG} - Holds the instruction and the operand that comes
        with the instruction
\end{itemize}

These two registers are about the minimum that we can get away with. Now,
we have two parts: the \textbf{function} and the \textbf{operand}. These
are both 4-bits long, meaning that each instruction is 8-bits long. In
terms of the instructions, there are three or four different classes of
instructions:
\begin{itemize}
    \item Load
    \begin{itemize}
        \item LDAM - load from A
        \item LDBM - Load from B
        \item STAM - Store in A
    \end{itemize}
    \item Load (constants)
    \begin{itemize}
        \item LDAC - Load A with a constant
        \item LDBC - Load B with a constant
        \item LDAP - Load address in program: This means that we can
            supply an address of the program as an operand, and store it
            in a register, and this means that we can use it as a
            callback.
    \end{itemize}
    \item Indirect loads
    \begin{itemize}
        \item LDAI - Load A indirectly
        \item LDBI - Load B indirectly
        \item STAI - Store A indirectly
    \end{itemize}
    \item Arithmetic operations and branches
    \begin{itemize}
        \item ADD - Adds
        \item SUB - subtracts
        \item BR - Branch always
        \item BRZ - Branch if zero
        \item BRN - Branch if negative
        \item BRB - Branch to the contents of the B register
    \end{itemize}
\end{itemize}

We now have 15 instructions, so we have one more instruction: PFIX, which
is a prefix. This means that we are able to encode a 16-bit value by first
supplying the PFIX with the first half, and then the function with the
second half. It attaches the operand to the operand of the next
instructions. Even this instruction set is able to do some quite
complicated things, such as actually be a compiler. 


\end{document}
