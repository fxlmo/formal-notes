%%=====================================================================================
%%
%%       Filename:  data-structs-and-algs.tex
%%
%%    Description:  Formal notes of DS and A
%%
%%        Version:  1.0
%%        Created:  02/10/19
%%       Revision:  none
%%
%%         Author:  Josh Felmeden (), nk18044@bristol.ac.uk
%%   Organization:  
%%      Copyright:  Copyright (c) 2019, Josh Felmeden
%%
%%          Notes:  
%%
%%=====================================================================================

% Preamble {{{
\documentclass[11pt,a4paper,titlepage,dvipsnames,cmyk]{scrartcl}
\usepackage[english]{babel}
\typearea{12}
% }}}

% Set indentation and line skip for paragraph {{{
\setlength{\parindent}{0em}
\setlength{\parskip}{1em}
\usepackage[margin=2cm]{geometry}
\addtolength{\textheight}{-1in}
\setlength{\headsep}{.5in}
% }}}

\usepackage{hhline} 
\usepackage{mathtools} 
\usepackage[T1]{fontenc}

% Headers setup {{{
\usepackage{fancyhdr}
\pagestyle{fancy}
\lhead{Data Structures and Algorithms: The formal notes}
\rhead{Josh Felmeden}
\usepackage{hyperref} 
% }}}

% Listings {{{
\usepackage[]{listings,xcolor} 
\lstset
{
    breaklines=true,
    tabsize=3,
    showstringspaces=false
}

\definecolor{lstgrey}{rgb}{0.05,0.05,0.05}
\usepackage{listings}
\makeatletter
\lstset{language=[Visual]Basic,
    backgroundcolor=\color{lstgrey},
    frame=single,
    xleftmargin=0.7cm,
    frame=tlbr, framesep=0.2cm, framerule=0pt,
    basicstyle=\lst@ifdisplaystyle\color{white}\footnotesize\ttfamily\else\color{black}\footnotesize\ttfamily\fi,
    captionpos=b,
    tabsize=2,
    keywordstyle=\color{Magenta}\bfseries,
    identifierstyle=\color{Cyan},
    stringstyle=\color{Yellow},
    commentstyle=\color{Gray}\itshape
}
\makeatother
\renewcommand{\familydefault}{\sfdefault}
% }}}


% Other packages {{{
\usepackage{amssymb}
\usepackage{graphicx}
\graphicspath{ {./pics/} }
\usepackage{needspace}
\usepackage{tcolorbox}
\usepackage{soul}
\usepackage{babel,dejavu,helvet} 
\usepackage{amsmath} 
\usepackage{booktabs} 
\usepackage{tcolorbox} 
\usepackage[symbol]{footmisc} 
\renewcommand{\thefootnote}{\fnsymbol{footnote}}
\renewcommand{\familydefault}{\sfdefault}
% }}}

% tcolorbox {{{
\newtcolorbox{blue}[3][] {
    colframe = #2!25,
    colback = #2!10,
    #1,
}

\newtcolorbox{titlebox}[3][] {
    colframe = #2!25,
    colback = #2!10,
    coltitle = #2!20!black,
    title = {#3},
    fonttitle=\bfseries
    #1,
}
% }}}

% Title {{{
\title{Data Structures and Algorithms: The formal notes}
\author{Josh Felmeden}
% }}}

\begin{document}

\maketitle
\tableofcontents

\newpage
\section{Greedy algorithms}%
\label{sec:Greedy algorithms}
\subsection{Interval scheduling}%
\label{sub:interval-scheduling}
Let's suppose that you're running a satellite imaging service. Taking a
satellite picture of an area isn't instant and can take some time. It can
also only be done on the day where the satellite's orbit is lined up
correctly. Say you want to request some images to be taken from said
satellite, each of which can only be taken at certain times and you can
only take one picture at a time. How do we satisfy as many requests as
possible?

The requested satellite times that we have to deal with are: 12:00-12:30,
12:05-12:20, 12:15-12:55, 12:20-12:25, 12:38-12:50, and 12:45-13:00.

If we visualise this in a graph, we get: 


\begin{center}
    \includegraphics[scale=.5]{graph-satellite.png}
\end{center}

If we take a greedy algorithm approach to assigning these slots, we could
do something like assign the slot that finishes earliest, and then repeat
doing this until we have reached the end. For example, the slot that
finishes fastest is 12:05-12:20, so we assign this. This now removes the
ability for both 12:10-12:30 and 12:15-12:55 to be assigned, so we remove
these. This continues until we end up with something looking like this:


\begin{center}
    \includegraphics[scale=.5]{graph-assigned}
\end{center}

This means that we satisfy four requests (which is actually the maximum
possible, so well done us).

We can formalise this by saying that a \textbf{request} is a pair of
integers $(s,f)$ with $0 \le s \le f$.

\newpage
The algorithm that we're left with is this:
\begin{lstlisting}
public sub greedySchedule
    sort R
    for each i in {1 ... n} do
        if s_i >= lastf then
            A.append(s_i, f_i)
            lastf = f_i
        end if
    next
end sub
\end{lstlisting}

Now, we need to prove that the output is actually a \textbf{compatible
subset} of R. This is sort of intuitive because the set we added doesn't
break compatibility, since $s \ge$ \lstinline|lastf| and \lstinline|lastf|
is the latest finish time that's already in $A$.

We can formalise this with a loop invariant. At the start of the $i$th
iteration, we see that
\begin{itemize}
    \item $A$ contains a compatible subset $\{(S_1,F_1), \dots,
        (S_t,F_t)\}$ of R.
    \item \lstinline|lastf| = $\max(\{0\} \cup \{F_j : j \le t\})$ 
\end{itemize}

The base case ($i=0$) is immediate because $A = []$. The induction step is
that $A$ was compatible at the start of the iteration, and therefore if we
append a pair $s_i,f_i$ to $A$ then $s \ge $ \lstinline|lastf| $\ge F_j$
for all $f \le t$. This means that $(s_i,f_i)$ is compatible with $A$.


\begin{titlebox}{blue}{Greedy algorithms definition}
Greedy algorithms are actually an informal term and people have different
definitions. The definition that we're going to use is:
\begin{itemize}
    \item They start with a sub-optimal solution.
    \item They look over all the possible improvements and pick the one
        that looks the best at the time.
    \item They never backtrack in `quality'.
\end{itemize}
\end{titlebox}

Greedy algorithms might fail; it's not enough to just do the obvious thing
at each stage. While the algorithms might fail initially, we can use the
knowledge that we gained from the results of the algorithm to design a
more correct one.

\section{Graphs}%
\label{sec:Graphs}

\begin{titlebox}{blue}{Graph Definition}
        A \textbf{graph} is a pair $G = (V,E)$ where $V=V(G)$ is a set of
        \textbf{vertices} $E = E(G)$ is a set of \textbf{edges} contained
        in $\{\{u,v\} : u,v \in V, u \not = \}$
\end{titlebox}

\begin{titlebox}{blue}{Walk Definition}
        A \textbf{walk} in a graph $G = (V, E)$ is a sequence of vertices
        such that $\{v_i, v_{i+1} \in E \text{ for all } i\le k-1$

        We say that the walk is from $v_0$ to $v_k$ and call $k$ the
        length of the walk.
\end{titlebox}


\subsection{Euler walks}%
\label{sub:Euler walks}
An \textbf{Euler walk} is one that contains every edge in $G$
exactly once.

Two graphs might be \textbf{equal}. This is the case when two graphs $G_1
= (V_1, E_1)$ and $G_2 = (V_2, E_2)$ are equal and written $G_1 = G_2$ if
$V_1 = V_2$ and $E_1 = E_2$. This does present some issues, however,
because sometimes graphs look like they should be equal, when they're not
because the edges are labelled differently.

This is where \textbf{isomorphism} comes in. $G_1$ and $G_2$ are
\textbf{isomorphic} if there's a bijection $f: V_1 \rightarrow V_2$ such
that $\{f(u),f(v) \} \in E_2$ if and only if $\{u,v\} \in E_1$.

Intuitively, this means that $G_1 \xrightarrow{\sim} G_2$ if they are the
same graph but the vertices are relabelled.

In a graph $G = (V,E)$, the \textbf{neighbourhood} of a vertex $v$ is the
set of vertices joined to $v$ by an edge. Formally, 
$N_G(v) = \{w \in V : \{v,w\} \in E$. Also, for all sets of vertices
$X \subseteq V = \cup_{v\in X} N_G(v)$

The \textbf{degree} of a vertex $v$ is the \textbf{number} of vertices
joined to $v$. Formally: $d_G(v) = |N_G(v)|$

\textbf{Theorem}: If $G$ has an Euler walk, then either:
\begin{itemize}
    \item Every vertex of $G$ has even degree or
    \item All but two vertices $v_0$ and $v_k$ have even degree, and any
        Euler walk must have $v_0$ and $v_k$ as endpoints.
\end{itemize}

Does every single graph that satisfies both of these conditions have an
Euler walk? No, because the graphs need to be \textbf{connected}.

Within a graph, we also have subgraphs and induced subgraphs. A
\textbf{subgraph} $H = (V_H, E_H)$ of $G$ is a graph with $V_H \subseteq$
and $E_H \subseteq E$. $H$ is an \textbf{induced subgraph} if $V_H
\subseteq V$ and $E_H = \{ e \in E : e \subseteq V_H\}$.

For all vertex sets $X \subseteq V$, the graph is \textbf{induced} by $X$
is:

\begin{center}
    \begin{align*}
        G[X] = (X, \{e \in E: e \subseteq X \})
    \end{align*}
\end{center}

A \textbf{component} $H$ of $G$ is the maximal connected induced subgraph
of $G$, so $H = G[V_H]$ is connected, but $G[V_H \cup \{v\}]$ is
disconnected for all $v \in V \backslash V_H$.

\begin{blue}{blue}
    \textbf{Theorem}: Let $G = (V,E)$ be a \textbf{connected} graph, and
    let $u,v \in V$. Then, $G$ has an Euler walk from $u$ to $v$ if and
    only if either:
    \begin{enumerate}
        \item $u = v$ and every vertex of $G$ has even degree, or
        \item $u \not = v$ and every vertex of $G$ has even degree except
            $u$ and $v$.
    \end{enumerate}
\end{blue}

\section{Sequential Processes}%
\label{sec:sequential-processes}

\section{Fast Fourier Transform}%
\label{sec:fft}

\subsection{Polynomials}%
\label{sub:Polynomials}
A \textbf{degree} $n-1$ polynomial in $x$ can be seen as a function:

\begin{align*}
    A(x) = \sum^{n-1}_{i=0}a_i \cdot x^i
\end{align*}

Any integer that's bigger than the degree of a polynomial is a
\textit{degree bound} of said polynomial. The polynomial $A$ is:

\begin{align*}
    a_0 \cdot x^0 + a_1 \cdot x^1 + a_2 \cdot x^2 \cdots + a_{n-1}x^{n-1}
\end{align*}

The values $a_i$ are the \textit{coefficients}, the degree is $n-1$ and
$n$ is a degree bound. We're able to express any integer as some kind of
polynomial by setting $x$ to some base, say for decimal numbers:

\begin{align*}
    A = \sum^{n-1}_{i=0} a_i \cdot 10^i
\end{align*}

The variable $x$ just allows us to evaluate the polynomial at a point. A
really fast way to evaluate the polynomial is to use \textbf{Horner's
Rule}.

\begin{titlebox}{blue}{Horner's Rule}
Instead of computing all the terms individually, we do:
\begin{align*}
    A(3) = a_0 + 3 \cdot (a_1 + 3\cdot (a_2 + \cdots + 3\cdot (a_{n-1})))
\end{align*}

This method requires $O(n)$ operations. For example, if we consider $A(x) = 2 + 3x + 1.x^2$, we can evaluate this
as:
\begin{align*}
    A(x) = 2 + x(3 + 1.x)
\end{align*}

\end{titlebox}

Once we have our polynomial representations, we might be doing some
arithmetic with them. We're allowed to write polynomials in a
\textit{coefficient representation}. Here, the addition of $C = A + B$
constructs $C$ as the vector:

\begin{align*}
    (a_0 + b_0, a_1 + b_1, a_2 + b_2, \dots, a_{n-1} + b_{n-1})
\end{align*}

$A$ and $B$ should really have the same length, but we're allowed to just
pad the coefficients with zero to make this the case.

\subsubsection{Point value Representation}%
\label{ssub:point-value}
We know that if we're given a polynomial, we can graph it. We can use this
fact to represent a polynomial as a list of its points. For point value
representation, the addition $C = A + B$ constructs $C$ as:

\begin{align*}
    \{x_0, y_0 + z_0),(x_1,y_1 + z_1),(x_2,y_2 + z_2), \dots,
        (x_{n-1},y_{n-1} + z_{n-1})
\end{align*}

where $x_i$ is a point, $y_i = A(x_i) and z_i = B(x_i)$.

It's important to note that the two value representations \textbf{must}
use the same evaluation points. Both of the operations are $O(n)$ in
terms of how long they take.

\subsubsection{Polynomial multiplication}%
\label{ssub:convolution}
Computing a polynomial multiplication (also called \textbf{convolution})
is a little harder than addition. It does, however, become much easier
when we use point value representation:

\begin{align*}
    \{x_0, y_0 \cdot z_0),(x_1,y_1 \cdot z_1),(x_2,y_2 \cdot z_2), \dots,
        (x_{n-1},y_{n-1} \cdot z_{n-1})
\end{align*}

where $x_i$ is a point, $y_i = A(x_i) and z_i = B(x_i)$.

The normal method of calculating multiplication is $O(n^2)$, while using
point value representation only takes $O(n)$. So, is there an easy way to
convert to point value representation? Actually, yes. What we need to do
is to evaluate the polynomial to a point-value representation, multiply
and finally interpolate (the opposite of evaluating) back again.

Long story short, we need to develop two fast algorithms that construct
the coefficients for the point value representation and then
interpolate. So the main steps to multiply the two polynomials $A$ and $B$
(of degree $n$) are:

\begin{enumerate}
    \item \textit{Double degree bound}: Create coefficient representations
        of $A(x)$ and $B(x)$ as degree bound $2n$ polynomials by adding
        $n$ high-order zero coefficients to each.
    \item \textit{Evaluate}: Compute point-value representations of $A(x)$
        and $B(x)$ of length $2n$ through two applications of FFT of order
        $2n$
    \item \textit{Pointwise multiply}: compute a point-value
        representation of $cC(x) = A(x)B(x)$ by multiplying the values
        Pointwise
    \item \textit{Interpolate}: Create a coefficient representation of
        $C(x)$ through a single application of the \textit{inverse} FFT.
\end{enumerate}

The first and third steps are really easy to perform in $O(n)$ time. The
claim is that if we evaluate at the complex roots of unity then we can
perform steps 2 and 4 in $O(n \log n)$ time.

\subsection{Evaluation at roots of unity}%
\label{sub:roots-unity}
First, we need to look at evaluation. We need to evaluate a polynomial of
degree $n$ at $n$ different points. It appears that the complexity of our
method is just going to be $O(n^2)$, but there is a faster way of doing it
than just using Horner's rule. This is when we evaluate the points at
\textit{special} points (namely the \textbf{N-th complex roots of unity}).

\begin{titlebox}{blue}{N-th complex roots of unity}
    \begin{itemize}
        \item The roots of unity are the values $\omega_N = e^{2\pi i j /
            N}$ for $j = 0,1,\dots,N-1$
        \item Say we are evaluating at $N$ pints, we take the N-th complex
            roots of unity $\omega_N$
        \item This means we evaluate the polynomial at the points:
            \begin{align*}
                \omega_N^0,\omega_N^1,\omega_N^2,\dots,\omega_N^{n-1}
            \end{align*}
    \end{itemize}
\end{titlebox}

We want to evaluate a polynomial A at the $n$ roots of unity. Therefore,
we evaluate:
\begin{align*}
    A(x) = \sum_{j=0}^{n-1}a_j \omega_n^{kj}
\end{align*}

for every $k = 0,1,\dots, n-1$

We define the vector of results of these evaluations as:

\begin{align*}
    y_k = A(\omega^k_n)
\end{align*}

This vector $y = (y_0, \dots, y_{n-1})$ is the \textbf{discrete Fourier
Transform} (DFT) of the coefficient vector $a = (a_0, a_1, \dots, a_{n-1})$

\begin{titlebox}{blue}{The Cancellation Lemma}
\begin{align*}
    \omega^{dk}_{dN} = \omega^{k}_{N}
\end{align*}
\end{titlebox}

\begin{titlebox}{blue}{The Halving Lemma}
    If $N > 0$ is even then the squares of the $N$ complex N-th roots of
    unity are the $N/2$ complex $N/2$-th roots of unity

    The proof of this is that we have $(\omega^k_n) ^2 = \omega^k_{n/2}$
    for any nonnegative integer $k$ because of the cancellation lemma.
\end{titlebox}

\subsection{Fast Fourier Transform}%
\label{sub:fft}
The really basic idea of the Fast Fourier transform is that we define two
new polynomials;

\begin{align*}
    A^{[0]}(x) = a_0 + a_2x + \cdots + a_{N-2}x^{N/2 - 1} \\
    A^{[1]}(x) = a_1 + a_3x + \cdots + a_{N-1}x^{N/2 - 1}
\end{align*}










\end{document}
